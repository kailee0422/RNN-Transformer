{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7c788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [05:03<00:00,  2.17it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:04<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 — Train Loss: 30.7040, Valid F1: 0.1000\n",
      "  Validation Precision: 0.1371, Recall: 0.0787\n",
      "  Saved new best model (F1: 0.1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [02:33<00:00,  4.28it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:04<00:00, 20.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 — Train Loss: 14.5786, Valid F1: 0.5482\n",
      "  Validation Precision: 0.5707, Recall: 0.5274\n",
      "  Saved new best model (F1: 0.5482)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [02:35<00:00,  4.23it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:04<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 — Train Loss: 6.6345, Valid F1: 0.6940\n",
      "  Validation Precision: 0.6799, Recall: 0.7087\n",
      "  Saved new best model (F1: 0.6940)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [02:33<00:00,  4.27it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:04<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 — Train Loss: 3.7665, Valid F1: 0.7353\n",
      "  Validation Precision: 0.7572, Recall: 0.7147\n",
      "  Saved new best model (F1: 0.7353)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [03:26<00:00,  3.19it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 — Train Loss: 2.5707, Valid F1: 0.7686\n",
      "  Validation Precision: 0.7615, Recall: 0.7759\n",
      "  Saved new best model (F1: 0.7686)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [06:15<00:00,  1.75it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 — Train Loss: 1.8379, Valid F1: 0.7685\n",
      "  Validation Precision: 0.7435, Recall: 0.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [06:14<00:00,  1.75it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 — Train Loss: 1.5190, Valid F1: 0.7774\n",
      "  Validation Precision: 0.7587, Recall: 0.7971\n",
      "  Saved new best model (F1: 0.7774)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [06:16<00:00,  1.75it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 — Train Loss: 1.2637, Valid F1: 0.7768\n",
      "  Validation Precision: 0.7506, Recall: 0.8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [06:12<00:00,  1.76it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 — Train Loss: 1.0275, Valid F1: 0.7867\n",
      "  Validation Precision: 0.7842, Recall: 0.7892\n",
      "  Saved new best model (F1: 0.7867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 657/657 [06:12<00:00,  1.76it/s]\n",
      "Evaluating: 100%|██████████| 83/83 [00:09<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 — Train Loss: 0.9225, Valid F1: 0.7886\n",
      "  Validation Precision: 0.7791, Recall: 0.7984\n",
      "  Saved new best model (F1: 0.7886)\n",
      "\n",
      "Loading best model for test evaluation...\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 83/83 [00:10<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Secbert-BiLstm-CRF\n",
      "================================================== \n",
      "\n",
      "Test Results - Precision: 0.8246, Recall: 0.8450, F1: 0.8347\n",
      "\n",
      "Detailed Test Results by Entity Type:\n",
      "Area 0.82 0.89 0.86\n",
      "Exp 0.98 0.99 0.99\n",
      "Features 0.96 0.96 0.96\n",
      "HackOrg 0.80 0.79 0.79\n",
      "Idus 0.83 0.92 0.87\n",
      "OffAct 0.80 0.81 0.80\n",
      "Org 0.70 0.72 0.71\n",
      "Purp 0.79 0.97 0.87\n",
      "SamFile 0.97 0.77 0.86\n",
      "SecTeam 0.91 0.86 0.89\n",
      "Time 0.89 0.90 0.90\n",
      "Tool 0.65 0.76 0.70\n",
      "Way 0.92 0.96 0.94\n",
      "Test Results - Precision: 0.8246, Recall: 0.8450, F1: 0.8347\n",
      "Detailed results saved to model_output/test_results.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import f1_score, classification_report as seqeval_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchcrf  import CRF\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data preprocessing\n",
    "def read_conll_format(file_path):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence = []\n",
    "            sentence_tags = []\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Split by spaces\n",
    "                    parts = line.split(' ')\n",
    "                    # Remove empty strings\n",
    "                    parts = [part for part in parts if part]\n",
    "                    if len(parts) >= 2:\n",
    "                        token = parts[0]\n",
    "                        tag = parts[-1]\n",
    "                        sentence.append(token)\n",
    "                        sentence_tags.append(tag)\n",
    "                else:\n",
    "                    if sentence:  # Skip empty sentences\n",
    "                        sentences.append(sentence)\n",
    "                        tags.append(sentence_tags)\n",
    "                        sentence = []\n",
    "                        sentence_tags = []\n",
    "            \n",
    "            # Add the last sentence if it's not empty\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(sentence_tags)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        raise\n",
    "            \n",
    "    return sentences, tags\n",
    "\n",
    "# Create mappings for tags to indices and vice versa\n",
    "def create_tag_mappings(tags_list):\n",
    "    unique_tags = set()\n",
    "    for tags in tags_list:\n",
    "        unique_tags.update(tags)\n",
    "    \n",
    "    tag2idx = {tag: idx for idx, tag in enumerate(sorted(unique_tags))}\n",
    "    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    \n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "# Custom Dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        \n",
    "        # Tokenize the sentence\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create tag sequence with -100 as padding/special tokens\n",
    "        tag_ids = torch.full((self.max_len,), fill_value=self.tag2idx['O'], dtype=torch.long)\n",
    "        \n",
    "        # Map word pieces to tags\n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                # Special tokens like [CLS], [SEP], [PAD]\n",
    "                continue\n",
    "            elif word_idx < len(tags):\n",
    "                # Only assign tags to first subword of each word\n",
    "                if i == 0 or word_ids[i-1] != word_idx:\n",
    "                    tag_ids[i] = self.tag2idx[tags[word_idx]]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': tag_ids,\n",
    "            'word_ids': word_ids\n",
    "        }\n",
    "\n",
    "# Model Architecture\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_tags):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=256, num_layers=2,\n",
    "            bidirectional=True, batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, num_tags)\n",
    "        # 用 torchcrf\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_out = self.dropout(outputs.last_hidden_state)\n",
    "        lstm_out, _ = self.bilstm(seq_out)\n",
    "        logits = self.classifier(lstm_out)  # (batch, seq_len, num_tags)\n",
    "\n",
    "        mask = attention_mask.bool()\n",
    "\n",
    "        if labels is not None:\n",
    "            # CRF loss: torchcrf 預設輸出的是 log-likelihood，要取負號\n",
    "            loss = -self.crf(logits, labels, mask=mask, reduction='mean')\n",
    "            return loss, logits\n",
    "        else:\n",
    "            # CRF decode -> list of [seq_len] tag idx\n",
    "            best_paths = self.crf.decode(logits, mask=mask)\n",
    "            return best_paths, logits\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, idx2tag, device, detailed=False):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            ids    = batch['input_ids'].to(device)\n",
    "            mask   = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 直接从模型获取best_paths\n",
    "            best_paths, _ = model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "            # 对齐 subword -> word\n",
    "            for i in range(ids.size(0)):\n",
    "                wids = batch['word_ids'][i]\n",
    "                prev = None\n",
    "                pred_seq, true_seq = [], []\n",
    "                \n",
    "                for j, widx in enumerate(wids):\n",
    "                    # 跳过特殊tokens和重复的word_id\n",
    "                    if widx is None or widx == prev:\n",
    "                        continue\n",
    "                    \n",
    "                    # 跳过超出长度的预测或padding\n",
    "                    if j >= len(best_paths[i]) or j >= mask[i].sum().item():\n",
    "                        continue\n",
    "                        \n",
    "                    # 获取预测标签和真实标签\n",
    "                    pred_tag = idx2tag[best_paths[i][j]]\n",
    "                    true_tag = idx2tag[labels[i, j].item()]\n",
    "                    \n",
    "                    pred_seq.append(pred_tag)\n",
    "                    true_seq.append(true_tag)\n",
    "                    prev = widx\n",
    "                \n",
    "                # 只添加非空序列\n",
    "                if pred_seq and true_seq:\n",
    "                    all_preds.append(pred_seq)\n",
    "                    all_trues.append(true_seq)\n",
    "\n",
    "    # 生成报告 dict\n",
    "    report = seqeval_report(all_trues, all_preds, output_dict=True, zero_division=0)\n",
    "    micro = report['micro avg']\n",
    "    precision = micro['precision']\n",
    "    recall    = micro['recall']\n",
    "    f1        = micro['f1-score']\n",
    "\n",
    "    if detailed:\n",
    "        print(\"=\"*50)\n",
    "        print(\"Secbert-BiLstm-CRF\")\n",
    "        print(\"=\"*50, \"\\n\")\n",
    "        print(f\"Test Results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")\n",
    "        print(\"Detailed Test Results by Entity Type:\")\n",
    "        \n",
    "        # 获取所有实体标签（包括所有B-, I-, S-, E-前缀）\n",
    "        for tag, m in sorted(report.items()):\n",
    "            if tag in ['O','macro avg','micro avg','weighted avg']:\n",
    "                continue\n",
    "            print(f\"{tag} {m['precision']:.2f} {m['recall']:.2f} {m['f1-score']:.2f}\")\n",
    "        return precision, recall, f1, report\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch]),\n",
    "        'word_ids': [x['word_ids'] for x in batch]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(train_losses, val_f1s, save_path='training_history.png'):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation F1\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_f1s, label='Validation F1')\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Create output directory\n",
    "    output_dir = \"model_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    train_sents, train_tags = read_conll_format('data/train.txt')\n",
    "    valid_sents, valid_tags = read_conll_format('data/valid.txt')\n",
    "    test_sents,  test_tags  = read_conll_format('data/test.txt')\n",
    "\n",
    "    # Prepare tag mappings & tokenizer\n",
    "    tag2idx, idx2tag = create_tag_mappings(train_tags + valid_tags + test_tags)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n",
    "\n",
    "    # Datasets & loaders\n",
    "    train_ds = NERDataset(train_sents, train_tags, tokenizer, tag2idx, max_len=256)\n",
    "    valid_ds = NERDataset(valid_sents, valid_tags, tokenizer, tag2idx, max_len=256)\n",
    "    test_ds  = NERDataset(test_sents,  test_tags,  tokenizer, tag2idx, max_len=256)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = NERModel('jackaduma/SecBERT', len(tag2idx)).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    best_f1 = 0\n",
    "    train_losses, val_f1s = [], []\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train(model, train_loader, optimizer, device)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        valid_precision, valid_recall, valid_f1 = evaluate(model, valid_loader, idx2tag, device, detailed=False)\n",
    "        val_f1s.append(valid_f1)\n",
    "        scheduler.step(valid_f1)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} — Train Loss: {loss:.4f}, Valid F1: {valid_f1:.4f}\")\n",
    "        print(f\"  Validation Precision: {valid_precision:.4f}, Recall: {valid_recall:.4f}\")\n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "            print(f\"  Saved new best model (F1: {valid_f1:.4f})\")\n",
    "\n",
    "    # Plot history\n",
    "    plot_training_history(train_losses, val_f1s, os.path.join(output_dir, 'training_history.png'))\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    print(\"\\nLoading best model for test evaluation...\")\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, 'best_model.pt')))\n",
    "\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_precision, test_recall, test_f1, test_report = evaluate(model, test_loader, idx2tag, device, detailed=True)\n",
    "    print(f\"Test Results - Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    # Save detailed test results\n",
    "    with open(os.path.join(output_dir, \"test_results.txt\"), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(\"Detailed Test Set Evaluation Results\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Test Results - Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\\n\\n\")\n",
    "        f.write(\"Detailed Test Results by Entity Type:\\n\")\n",
    "        f.write(f\"{'Entity':<10} {'Precision':>9} {'Recall':>7} {'F1-score':>9}\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        for tag in sorted(test_report.keys()):\n",
    "            if tag in ['O','macro avg','micro avg','weighted avg']:\n",
    "                continue\n",
    "            m = test_report[tag]\n",
    "            f.write(f\"{tag:<10} {m['precision']:>7.2f}    {m['recall']:>5.2f}    {m['f1-score']:>7.2f}\\n\")\n",
    "\n",
    "\n",
    "    print(f\"Detailed results saved to {output_dir}/test_results.txt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f81d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model: model_output\\best_model.pt\n",
      "Evaluating on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 83/83 [00:04<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Secbert-BiLstm-CRF\n",
      "================================================== \n",
      "\n",
      "Test results - Precision: 0.8246, Recall: 0.8450, F1: 0.8347\n",
      "\n",
      "Detailed test results by entity type:\n",
      "B-Area 0.86 0.92 0.89\n",
      "B-Exp 0.98 0.99 0.99\n",
      "B-Features 0.97 0.96 0.96\n",
      "B-HackOrg 0.84 0.81 0.83\n",
      "B-Idus 0.85 0.95 0.90\n",
      "B-OffAct 0.84 0.83 0.84\n",
      "B-Org 0.75 0.72 0.74\n",
      "B-Purp 0.84 0.99 0.91\n",
      "B-SamFile 0.98 0.77 0.86\n",
      "B-SecTeam 0.98 0.92 0.95\n",
      "B-Time 0.94 0.94 0.94\n",
      "B-Tool 0.69 0.78 0.73\n",
      "B-Way 0.96 0.97 0.97\n",
      "I-Area 0.77 0.90 0.83\n",
      "I-Exp 1.00 1.00 1.00\n",
      "I-Features 0.99 0.91 0.94\n",
      "I-HackOrg 0.77 0.77 0.77\n",
      "I-Idus 0.69 1.00 0.82\n",
      "I-OffAct 0.91 0.77 0.83\n",
      "I-Org 0.75 0.76 0.76\n",
      "I-Purp 0.81 1.00 0.90\n",
      "I-SamFile 1.00 0.85 0.92\n",
      "I-SecTeam 0.74 0.86 0.79\n",
      "I-Time 0.98 0.84 0.90\n",
      "I-Tool 0.69 0.78 0.73\n",
      "I-Way 0.94 0.98 0.96\n",
      "Detailed results saved to model_output/detailed_test_results.txt\n",
      "Test results - Precision: 0.8246, Recall: 0.8450, F1: 0.8347\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchcrf import CRF\n",
    "\n",
    "# Check CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data preprocessing\n",
    "def read_conll_format(file_path):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence = []\n",
    "            sentence_tags = []\n",
    "            \n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Split by whitespace\n",
    "                    parts = line.split(' ')\n",
    "                    # Remove empty strings\n",
    "                    parts = [part for part in parts if part]\n",
    "                    if len(parts) >= 2:\n",
    "                        token = parts[0]\n",
    "                        tag = parts[-1]\n",
    "                        sentence.append(token)\n",
    "                        sentence_tags.append(tag)\n",
    "                else:\n",
    "                    if sentence:  # Skip empty sentences\n",
    "                        sentences.append(sentence)\n",
    "                        tags.append(sentence_tags)\n",
    "                        sentence = []\n",
    "                        sentence_tags = []\n",
    "            \n",
    "            # Add the last sentence (if not empty)\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(sentence_tags)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        raise\n",
    "            \n",
    "    return sentences, tags\n",
    "\n",
    "# Create tag mappings\n",
    "def create_tag_mappings(tags_list):\n",
    "    unique_tags = set()\n",
    "    for tags in tags_list:\n",
    "        unique_tags.update(tags)\n",
    "    \n",
    "    tag2idx = {tag: idx for idx, tag in enumerate(sorted(unique_tags))}\n",
    "    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    \n",
    "    return tag2idx, idx2tag\n",
    "\n",
    "# Custom dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, tag2idx, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        \n",
    "        # Tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create tag sequence, fill with the index of the 'O' tag\n",
    "        tag_ids = torch.full((self.max_len,), fill_value=self.tag2idx['O'], dtype=torch.long)\n",
    "        \n",
    "        # Map subwords to tags\n",
    "        word_ids = encoding.word_ids()\n",
    "        \n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                # Special tokens like [CLS], [SEP], [PAD]\n",
    "                continue\n",
    "            elif word_idx < len(tags):\n",
    "                # Assign tags only to the first subword of each word\n",
    "                if i == 0 or word_ids[i-1] != word_idx:\n",
    "                    tag_ids[i] = self.tag2idx[tags[word_idx]]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': tag_ids,\n",
    "            'word_ids': word_ids\n",
    "        }\n",
    "\n",
    "# Model architecture\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_tags):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.bert.config.hidden_size,\n",
    "            hidden_size=256, num_layers=2,\n",
    "            bidirectional=True, batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.classifier = nn.Linear(512, num_tags)\n",
    "        # Use torchcrf\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_out = self.dropout(outputs.last_hidden_state)\n",
    "        lstm_out, _ = self.bilstm(seq_out)\n",
    "        logits = self.classifier(lstm_out)  # (batch, seq_len, num_tags)\n",
    "\n",
    "        mask = attention_mask.bool()\n",
    "\n",
    "        if labels is not None:\n",
    "            # CRF loss: torchcrf outputs log-likelihood by default, take the negative\n",
    "            loss = -self.crf(logits, labels, mask=mask, reduction='mean')\n",
    "            return loss, logits\n",
    "        else:\n",
    "            # CRF decoding -> list of [seq_len] tag indices\n",
    "            best_paths = self.crf.decode(logits, mask=mask)\n",
    "            return best_paths, logits\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'labels': torch.stack([x['labels'] for x in batch]),\n",
    "        'word_ids': [x['word_ids'] for x in batch]\n",
    "    }\n",
    "\n",
    "# Calculate metrics manually\n",
    "def calculate_metrics(true_tags, pred_tags):\n",
    "    \"\"\"Calculate precision, recall, f1 for each tag type\"\"\"\n",
    "    # Collect all unique tags\n",
    "    all_tags = set()\n",
    "    for seq in true_tags + pred_tags:\n",
    "        all_tags.update(seq)\n",
    "    \n",
    "    # Remove 'O' tag if present\n",
    "    if 'O' in all_tags:\n",
    "        all_tags.remove('O')\n",
    "    \n",
    "    tag_metrics = {}\n",
    "    \n",
    "    for tag in sorted(all_tags):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        \n",
    "        for true_seq, pred_seq in zip(true_tags, pred_tags):\n",
    "            true_tag_indices = [i for i, t in enumerate(true_seq) if t == tag]\n",
    "            pred_tag_indices = [i for i, t in enumerate(pred_seq) if t == tag]\n",
    "            \n",
    "            # True positive: in both true and predicted\n",
    "            tp += len(set(true_tag_indices) & set(pred_tag_indices))\n",
    "            \n",
    "            # False positive: in predicted but not in true\n",
    "            fp += len(set(pred_tag_indices) - set(true_tag_indices))\n",
    "            \n",
    "            # False negative: in true but not in predicted\n",
    "            fn += len(set(true_tag_indices) - set(pred_tag_indices))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        tag_metrics[tag] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1-score': f1\n",
    "        }\n",
    "    \n",
    "    return tag_metrics\n",
    "\n",
    "# Evaluation function with detailed breakdown\n",
    "def evaluate(model, dataloader, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            ids    = batch['input_ids'].to(device)\n",
    "            mask   = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Get best_paths directly from the model\n",
    "            best_paths, _ = model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "            # Align subword -> word\n",
    "            for i in range(ids.size(0)):\n",
    "                wids = batch['word_ids'][i]\n",
    "                prev = None\n",
    "                pred_seq, true_seq = [], []\n",
    "                \n",
    "                for j, widx in enumerate(wids):\n",
    "                    # Skip special tokens and repeated word_ids\n",
    "                    if widx is None or widx == prev:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip predictions or padding beyond length\n",
    "                    if j >= len(best_paths[i]) or j >= mask[i].sum().item():\n",
    "                        continue\n",
    "                        \n",
    "                    # Get predicted and true tags\n",
    "                    pred_tag = idx2tag[best_paths[i][j]]\n",
    "                    true_tag = idx2tag[labels[i, j].item()]\n",
    "                    \n",
    "                    pred_seq.append(pred_tag)\n",
    "                    true_seq.append(true_tag)\n",
    "                    prev = widx\n",
    "                \n",
    "                # Add only non-empty sequences\n",
    "                if pred_seq and true_seq:\n",
    "                    all_preds.append(pred_seq)\n",
    "                    all_trues.append(true_seq)\n",
    "\n",
    "    # Generate report dict - first use seqeval for overall metrics\n",
    "    seqeval_report_dict = seqeval_report(all_trues, all_preds, output_dict=True, zero_division=0)\n",
    "    micro = seqeval_report_dict['micro avg']\n",
    "    precision = micro['precision']\n",
    "    recall    = micro['recall']\n",
    "    f1        = micro['f1-score']\n",
    "\n",
    "    # Now get detailed metrics for each tag type including all prefixes\n",
    "    tag_metrics = calculate_metrics(all_trues, all_preds) \n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"Secbert-BiLstm-CRF\")\n",
    "    print(\"=\"*50, \"\\n\")\n",
    "    print(f\"Test results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")\n",
    "    print(\"Detailed test results by entity type:\")\n",
    "    \n",
    "    # Display all tag types (including B-, I-, E-, S- prefixes)\n",
    "    for tag, metrics in sorted(tag_metrics.items()):\n",
    "        print(f\"{tag} {metrics['precision']:.2f} {metrics['recall']:.2f} {metrics['f1-score']:.2f}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    output_dir = \"model_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"detailed_test_results.txt\"), 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(\"Detailed test evaluation results\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\\n\")\n",
    "        f.write(f\"Test results - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\\n\")\n",
    "        f.write(\"Detailed test results by entity type:\\n\")\n",
    "        \n",
    "        # Write all tag types, including B-, I-, E-, S- prefixes\n",
    "        for tag, metrics in sorted(tag_metrics.items()):\n",
    "            f.write(f\"{tag} {metrics['precision']:.2f} {metrics['recall']:.2f} {metrics['f1-score']:.2f}\\n\")\n",
    "\n",
    "    print(f\"Detailed results saved to {output_dir}/detailed_test_results.txt\")\n",
    "    \n",
    "    return precision, recall, f1, tag_metrics\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_sents, train_tags = read_conll_format('data/train.txt')\n",
    "    valid_sents, valid_tags = read_conll_format('data/valid.txt')\n",
    "    test_sents,  test_tags  = read_conll_format('data/test.txt')\n",
    "\n",
    "    # Prepare tag mappings and tokenizer\n",
    "    tag2idx, idx2tag = create_tag_mappings(train_tags + valid_tags + test_tags)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    test_ds = NERDataset(test_sents, test_tags, tokenizer, tag2idx, max_len=256)\n",
    "    test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Create model\n",
    "    model = NERModel('jackaduma/SecBERT', len(tag2idx)).to(device)\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    output_dir = \"model_output\"\n",
    "    model_path = os.path.join(output_dir, 'best_model.pt')\n",
    "    \n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Evaluate and generate detailed results\n",
    "    print(\"Evaluating on the test set...\")\n",
    "    test_precision, test_recall, test_f1, test_metrics = evaluate(model, test_loader, idx2tag, device)\n",
    "    \n",
    "    print(f\"Test results - Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
