## üîç Overview

This project explores the use of **Compared with LSTM and GRU** for classifying whether a tweet is about a real disaster or not. It is based on the [Kaggle NLP Disaster Tweets dataset](https://www.kaggle.com/competitions/nlp-getting-started/overview). The goal is to compare the performance of LSTM and GRU models in terms of classification accuracy, training speed, and resource usage.



## üß† Model Pipeline

### 1. **Data Preprocessing**
- Remove punctuation, HTML, and non-ASCII characters
- Normalize URLs, usernames, numbers, emojis, and common abbreviations
- Token replacements for consistent input to the models

### 2. **Model Architecture**
- **BERT-base-uncased** encoder (frozen during training)
- One-layer **LSTM** or **GRU** (hidden size: 256)
- Dropout layer (0.2)
- Fully connected layer + Sigmoid activation for binary output

### 3. **Training Setup**
- Loss: Binary Cross-Entropy
- Optimizer: Adam (lr=0.0001)
- Scheduler: ReduceLROnPlateau
- Epochs: 30, Batch size: 32
- Validation split: 20%

---

## üìä Evaluation Metrics
- **Accuracy**, **Precision**, **Recall**, **F1 Score**
- Training time and GPU memory usage also recorded
- Confusion matrices and training/validation curves provided for analysis



## üìà Results

###  Validation Set



###  Test Set


> ‚ö†Ô∏è *Due to the test set containing no positive samples, both models failed to predict any positive cases, leading to precision/recall = 0.*

###  Resource Usage



## üßæ Conclusion

GRU achieved higher accuracy and precision compared to LSTM, while also training slightly faster with fewer parameters. Although both models failed to identify positive samples in the test set, GRU showed better performance and efficiency overall, making it more suitable for this binary classification task.



## üöÄ How to Run

```bash
# Install dependencies
pip install -r requirements.txt

# Run LSTM training
python train_lstm.py

# Run GRU training
python train_gru.py

# Predict test data
python inference.py --model GRU
```


